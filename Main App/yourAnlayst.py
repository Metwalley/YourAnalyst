import streamlit as st
import pandas as pd
import all_function as af
import visualize as v
from io import BytesIO
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
import pickle
import os
import io
import numpy as np



st.markdown(
    """
    <style>
    /* ØªØ®ØµÙŠØµ Ø§Ù„Ø®Ù„ÙÙŠØ© Ø§Ù„Ø¹Ø§Ù…Ø© */
    .reportview-container {
        background-color: #212121;  /* Ø®Ù„ÙÙŠØ© Ø¯Ø§ÙƒÙ†Ø© */
        color: #e0e0fd;  /* Ù„ÙˆÙ† Ø§Ù„Ù†Øµ Ø§Ù„ÙØ§ØªØ­ */
    }

    /* ØªØ®ØµÙŠØµ Ø§Ù„Ø®Ù„ÙÙŠØ© Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ© */
    .sidebar .sidebar-content {
        background-color: #e0e0fd;  /* Ø®Ù„ÙÙŠØ© Ø¯Ø§ÙƒÙ†Ø© Ù„Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ */
        color: #e0e0fd;  /* Ù„ÙˆÙ† Ø§Ù„Ù†Øµ Ø§Ù„ÙØ§ØªØ­ */
    }

    /* ØªØ®ØµÙŠØµ Ø§Ù„Ø£Ø²Ø±Ø§Ø± */
    .stButton>button {
        cursor: pointer;
        border: solid rgb(187, 204, 0);
        font-family: "system-ui";
        font-size: 14px;
        color: rgb(255, 255, 255);
        padding: 10px 30px;
        transition: 2s;
        width: 335px;
        box-shadow: rgb(0, 0, 0) 0px 0px 0px 0px;
        border-radius: 50px;
        background: linear-gradient(90deg, rgb(0, 102, 204) 0%, rgb(197, 0, 204) 100%);
    }

    .stButton>button:hover {
        color: rgb(255, 255, 255);
        width: 337px;
        background: rgb(0, 102, 204) none repeat scroll 0% 0% / auto padding-box border-box;
        border-color: rgb(204, 0, 105);
        border-width: 1px;
        border-style: solid;
    }

    /* ØªØ®ØµÙŠØµ Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ØªØ¨ÙˆÙŠØ¨ */
    .css-1c6jbr4 {
        font-family: 'Helvetica Neue', sans-serif;
        font-size: 26px;
        font-weight: bold;
        color: #e0e0fd;  /* Ù„ÙˆÙ† Ø£Ø¨ÙŠØ¶ Ù„Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† */
    }

    /* ØªØ®ØµÙŠØµ Ø§Ù„ÙÙˆÙ†Øª Ø§Ù„Ø¹Ø§Ù… */
    body {
        font-family: 'Roboto', sans-serif;
    }

    /* ØªØ®ØµÙŠØµ Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª */
    .stDataFrame {
        background-color: #424242;  /* Ø®Ù„ÙÙŠØ© Ø¯Ø§ÙƒÙ†Ø© Ù„Ù„Ø¬Ø¯Ø§ÙˆÙ„ */
        color: #e0e0fd;  /* Ù„ÙˆÙ† Ø§Ù„Ù†Øµ Ø§Ù„ÙØ§ØªØ­ ÙÙŠ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ */
        border-radius: 10px;
        box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.3);
    }

    /* ØªØ®ØµÙŠØµ Ø§Ù„ØªØ±ÙˆÙŠØ³Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© */
    .stTitle {
        font-family: 'Arial', sans-serif;
        font-size: 34px;
        color: #e0e0fd;  /* Ù„ÙˆÙ† Ø§Ù„Ø£Ø²Ø±Ù‚ Ù„Ù„Ø£Ù„Ù‚Ø§Ø¨ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© */
        font-weight: bold;
    }

    /* ØªØ®ØµÙŠØµ Ø§Ù„ÙØ§ØµÙ„ Ø¨ÙŠÙ† Ø§Ù„ØªØ¨ÙˆÙŠØ¨Ø§Øª */
    .stTab {
        border: 2px solid #e0e0fd;
        border-radius: 10px;
        background-color: #333333; 
        padding: 10px;
    }

    /* ØªØ®ØµÙŠØµ Ø§Ù„ØªØ±ÙˆÙŠØ³Ø§Øª Ø¯Ø§Ø®Ù„ Ø§Ù„ØªØ¨ÙˆÙŠØ¨Ø§Øª */
    .stHeader {
        color: #e0e0fd;  
        font-size: 22px;
        font-weight: 600;
    }

    /* ØªØ®ØµÙŠØµ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø¯Ø§Ø®Ù„ Ø§Ù„ØªØ¨ÙˆÙŠØ¨Ø§Øª */
    .stSubheader {
        color: #e0e0fd;  /* Ù„ÙˆÙ† ÙØ§ØªØ­ Ù„Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„ÙØ±Ø¹ÙŠØ© */
        font-size: 18px;
    }

    </style>
    """,
    unsafe_allow_html=True
)





# ========== Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ==========
st.title("ğŸŒŸ YourAnalyst ")

# ========== ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¯Ø§ØªØ§ ==========
uploaded_file = st.file_uploader("Upload your dataset", type=["csv", "excel", "json", "parquet"])

if uploaded_file is not None:
    file_type = st.selectbox("Select file type", ["csv", "excel", "json", "parquet"])

    if "df" not in st.session_state:
        st.session_state.df = af.load_data(uploaded_file, file_type)

    df = st.session_state.df

    st.success("âœ… Dataset uploaded successfully!")

    # ========== Ø§Ù„ØªØ§Ø¨Ø² ==========
    tabs = st.tabs(["ğŸ› ï¸ Preprocessing", "ğŸ“Š Visualization", "ğŸ¤– Models"])

    # ==============================
    # 1. ØªØ¨ÙˆÙŠØ¨ Preprocessing
    # ==============================
    with tabs[0]:
        st.header("ğŸ› ï¸ Data Preprocessing")

        option = st.radio(
            "Select a preprocessing task:",
            (
                "ğŸ“„ View Dataset",
                "ğŸ“Œ List All Columns",
                "ğŸ“ˆ Column Info",
                "ğŸ§¾ Check Data Integrity",
                "ğŸ“Š Dataset Summary",
                "ğŸ§¹ Remove Duplicates",
                "ğŸ” Detect Missing Values",
                "ğŸ”§ Handle Missing Values",
                "ğŸ’¡ Encode Features",
                "ğŸ—‘ï¸ Remove Columns",
                "âœï¸ Rename Columns",
                "ğŸ§  Handle Outliers",
                "ğŸ”„ Replace Values",        # â† NEW
                "ğŸ” Change Column Data Types", # â† NEW
                "ğŸ•“ Extract Datetime Features", # â† NEW
                "ğŸ”¢ Scale/Normalize Features", # â† NEW prepFor ML
                "ğŸ§¬ Feature Interaction or Generation", # â† NEW prepFor ML
                "ğŸ§½ Detect & Handle Low-Variance Features", # â† NEW prepFor ML
                "ğŸ“‰ Correlation Analysis / Multicollinearity Detection", # â† NEW prepFor ML
                "ğŸ“Š Feature Importance", # â† NEW prepFor ML
                "ğŸ” Check Class Imbalance", # â† NEW prepFor ML
                "ğŸ¯ Set Target Variable", # â† NEW prepFor ML
                "ğŸ“¦ Split Dataset (Train/Test)", # â† NEW prepFor ML
                "ğŸ“¥ Download Cleaned Dataset"
            )
        )

        if option == "ğŸ“„ View Dataset":
            st.subheader("ğŸ“„ Dataset Preview")

            show_all = st.checkbox("ğŸ” Show full dataset", value=False)

            if show_all:
                st.dataframe(df)
            else:
                st.write("### ğŸ” First 10 Rows")
                st.dataframe(df.head(10))

                st.write("### ğŸ”š Last 10 Rows")
                st.dataframe(df.tail(10))


        elif option == "ğŸ“Œ List All Columns":
            af.list_all_columns(df)

        elif option == "ğŸ“ˆ Column Info":
            col = st.selectbox("Choose column:", df.columns)
            af.view_column_info(df, col)
        
        elif option == "ğŸ“Š Feature Importance":
            st.subheader("ğŸ“Š Feature Importance (via Random Forest)")

            target = st.selectbox("Select the target column:", df.columns)
            if target:
                importance = af.get_feature_importance(df, target)
                st.bar_chart(importance.head(20))
                st.dataframe(importance.reset_index().rename(columns={'index': 'Feature', 0: 'Importance'}))

        elif option == "ğŸ” Check Class Imbalance":
            st.subheader("ğŸ” Search for Class Imbalance")

            target_col = st.selectbox("Select the target column:", df.columns)
            class_counts = af.check_class_imbalance(df, target_col)

            st.bar_chart(class_counts)
            st.dataframe(class_counts.reset_index().rename(columns={'index': 'Class', target_col: 'Count'}))

            imbalance_ratio = class_counts.max() / class_counts.min() if class_counts.min() > 0 else float('inf')
            if imbalance_ratio > 1.5:
                st.warning(f"âš ï¸ Class imbalance detected (max/min ratio = {imbalance_ratio:.2f}). Consider balancing techniques like SMOTE.")
            else:
                st.success("âœ… Class distribution looks reasonably balanced.")

        elif option == "ğŸ¯ Set Target Variable":
            st.subheader("ğŸ¯ Select Target (Prediction) Column")

            target_col = st.selectbox("Select your target variable (what you're predicting):", df.columns)

            if st.button("Confirm Target Column"):
                st.session_state.target_col = target_col
                st.success(f"ğŸ¯ Target variable set to: `{target_col}`")

        elif option == "ğŸ“¦ Split Dataset (Train/Test)":
            st.subheader("ğŸ“¦ Split Dataset into Train/Test")

            if "target_col" not in st.session_state:
                st.warning("âš ï¸ Please set a target column first in the ğŸ¯ section.")
            else:
                test_size = st.slider("Test Size (%)", min_value=5, max_value=50, value=20, step=5)
                train_size = st.slider("Train Size (%)", min_value=10, max_value=90, value=80, step=5)
                stratify = st.checkbox("Use Stratified Split (for classification)?")

                if st.button("Split Dataset"):
                    X_train, X_test, y_train, y_test = af.split_dataset(
                    st.session_state.df,
                    st.session_state.target_col,
                    test_size=test_size / 100,
                    train_size=train_size / 100,
                    stratify=stratify
                )
                    st.session_state.X_train = X_train
                    st.session_state.X_test = X_test
                    st.session_state.y_train = y_train
                    st.session_state.y_test = y_test

                    st.success(f"âœ… Dataset split successfully: {len(X_train)} train rows, {len(X_test)} test rows.")
                    st.write("ğŸ“˜ **Train Set Preview**")
                    st.dataframe(X_train.head())

            
        elif option == "ğŸ•“ Extract Datetime Features":
            st.subheader("ğŸ•“ Extract Datetime Features")
            datetime_cols = df.select_dtypes(include=["datetime64[ns]", "object"]).columns
            datetime_col = st.selectbox("Select a datetime column:", datetime_cols)

            if st.button("Extract Features"):
                df = af.extract_datetime_features(df, datetime_col)
                st.session_state.df = df
                st.dataframe(df)
    
            
        elif option == "ğŸ” Change Column Data Types":
            st.subheader("ğŸ” Change Column Data Types")

            if st.checkbox("Select all columns"):
                columns = st.multiselect("Select columns:", df.columns, default=list(df.columns))
            else:
                columns = st.multiselect("Select columns:", df.columns)

            dtype_options = ["int", "float", "object", "datetime", "category"]
            new_dtype = st.selectbox("Convert to data type:", dtype_options)

            if st.button("Apply Type Conversion"):
                df = af.change_columns_dtype(df, columns, new_dtype)
                st.session_state.df = df
                st.dataframe(df)



        elif option == "ğŸ“Š Dataset Summary":
            af.view_all_columns_summary(df)

        elif option == "ğŸ§¹ Remove Duplicates":
            st.subheader("ğŸ§¹ Remove Duplicate Rows")
            num_duplicates = af.check_duplicates(df)

            if num_duplicates == 0:
                st.info("âœ… No duplicate rows found in the dataset.")
            else:
                st.warning(f"âš ï¸ Found {num_duplicates} duplicate rows in your dataset.")
                if st.button("ğŸ§¹ Remove Duplicates Now"):
                    df, removed = af.remove_duplicates(df)
                    st.session_state.df = df
                    st.success(f"âœ… Removed {removed} duplicate rows.")
                    st.dataframe(df)

        elif option == "ğŸ” Detect Missing Values":
            st.subheader("ğŸ” Missing Values")
            missing_cols = af.get_missing_columns(df)
            if not missing_cols:
                st.info("âœ… No missing values found in the dataset.")
            else:
                st.warning("âš ï¸ Missing values found in the following columns:")
                for col in missing_cols:
                    dtype = df[col].dtype
                    missing_count = df[col].isnull().sum()
                    st.markdown(f"- **{col}** *(dtype: `{dtype}`, missing: `{missing_count}`)*")

        elif option == "ğŸ“‰ Correlation Analysis / Multicollinearity Detection":
            st.subheader("ğŸ“‰ Correlation Analysis / Multicollinearity Detection")

            threshold = st.slider("Correlation Threshold (absolute)", min_value=0.5, max_value=1.0, value=0.9, step=0.01)
            to_drop, corr_matrix = af.detect_highly_correlated(df, threshold)

            st.write("Correlation Matrix:")
            st.dataframe(corr_matrix.style.background_gradient(cmap='coolwarm'))

            if not to_drop:
                st.info("âœ… No highly correlated features found.")
            else:
                st.warning(f"âš ï¸ {len(to_drop)} features exceed the correlation threshold.")
                st.write(to_drop)

                if st.button("Remove Correlated Features"):
                    df = af.remove_correlated_features(df, to_drop)
                    st.session_state.df = df
                    st.dataframe(df)



        elif option == "ğŸ§¬ Feature Interaction or Generation":
            st.subheader("ğŸ§¬ Feature Interaction / Generation")

            st.markdown("Example: `(df['Feature1'] + df['Feature2']) / 2`")
            cols = df.columns.tolist()

            with st.expander("â• Generate New Feature", expanded=True):
                col1 = st.selectbox("Select first column:", cols, key="gen_col1")
                col2 = st.selectbox("Select second column:", cols, key="gen_col2")

                operation = st.radio("Select operation:", ["add", "subtract", "multiply", "divide"], horizontal=True)
                new_col_name = st.text_input("Enter name for the new feature:", key="new_feature_name")

                col1_, col2_ = st.columns(2)
                with col1_:
                    if st.button("âœ… Generate Feature"):
                        if new_col_name.strip() == "":
                            st.error("âš ï¸ Please enter a valid name for the new feature.")
                        elif new_col_name in df.columns:
                            st.error("âš ï¸ A column with this name already exists.")
                        else:
                            # Save previous version before change
                            st.session_state.prev_df = df.copy()

                            # Apply feature creation
                            df = af.generate_feature(df, col1, col2, operation, new_col_name)
                            st.session_state.df = df
                            st.success("ğŸ‰ Feature created successfully!")
                            st.dataframe(df.head())

                with col2_:
                    if st.button("â†©ï¸ Undo Last Change"):
                        if "prev_df" in st.session_state:
                            st.session_state.df = st.session_state.prev_df.copy()
                            del st.session_state.prev_df
                            st.success("âœ… Reverted to previous version.")
                            st.dataframe(st.session_state.df.head())
                        else:
                            st.warning("âš ï¸ No previous version to revert to.")

        elif option == "ğŸ§½ Detect & Handle Low-Variance Features":
            st.subheader("ğŸ§½ Detect & Handle Constant or Low-Variance Features")

            threshold = st.slider("Variance Threshold", min_value=0.0, max_value=1.0, value=0.0, step=0.01)
            low_var_cols = af.detect_low_variance_features(df, threshold)

            if not low_var_cols:
                st.info("âœ… No low-variance features detected.")
            else:
                st.warning(f"âš ï¸ Found {len(low_var_cols)} low-variance columns.")
                st.write(low_var_cols)

                if st.button("Remove Low-Variance Features"):
                    df = af.remove_low_variance_features(df, low_var_cols)
                    st.session_state.df = df
                    st.dataframe(df)



        elif option == "ğŸ”§ Handle Missing Values":
            st.subheader("ğŸ”§ Handle Missing Values")
            missing_cols = af.get_missing_columns(df)

            if not missing_cols:
                st.info("âœ… No missing values found in the dataset.")
            else:
                selected_cols = st.multiselect(
                    "Select columns with missing values:", missing_cols
                )

                methods_dict = {}
                custom_values = {}

                for col in selected_cols:
                    col_type = df[col].dtype
                    st.markdown(f"**Column: `{col}` ({col_type})**")

                    # 1) Offer the same â€œCustomâ€ option for both numeric & non-numeric
                    if pd.api.types.is_numeric_dtype(df[col]):
                        options = ["Drop", "Mean", "Median", "Custom"]
                    else:
                        options = ["Drop", "Mode", "Custom"]

                    method = st.selectbox(
                        f"Choose method for `{col}`:", options, key=f"method_{col}"
                    )
                    methods_dict[col] = method

                    # 2) If user picks Custom, ask for the value
                    if method == "Custom":
                        custom_value = st.text_input(
                            f"Enter custom fill for `{col}`:", key=f"custom_{col}"
                        )
                        custom_values[col] = custom_value

                if st.button("Apply Missing Value Handling"):
                    for col in selected_cols:
                        df = af.handle_missing_column(
                            df,
                            col,
                            methods_dict[col],
                            custom_values.get(col)
                        )

                    st.session_state.df = df
                    st.success("âœ… Missing values handled successfully.")
                    st.dataframe(df)


        elif option == "ğŸ’¡ Encode Features":
            st.subheader("ğŸ’¡ Encode Features")

            cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()

            if st.checkbox("Select all categorical columns"):
                cols = st.multiselect("Select columns to encode", cat_cols, default=cat_cols)
            else:
                cols = st.multiselect("Select columns to encode", cat_cols)

            method = st.selectbox("Encoding method", ['label', 'onehot', 'ordinal'])

            if st.button("Apply Encoding"):
                df = af.encode_features(df, cols, method)
                st.session_state.df = df
                st.dataframe(df)



        elif option == "ğŸ—‘ï¸ Remove Columns":
            cols = st.multiselect("Select columns to remove", df.columns)
            if st.button("Remove Columns"):
                df = af.remove_columns(df, cols)
                st.session_state.df = df
                st.success("âœ… Columns removed.")
                st.dataframe(df)

        elif option == "âœï¸ Rename Columns":
            st.subheader("âœï¸ Rename Columns")

            # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø°ÙŠ Ø³ÙŠØªÙ… ØªØºÙŠÙŠØ±Ù‡
            col_to_rename = st.selectbox("Choose column to rename:", df.columns)

            # Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ù„Ù„Ø¹Ù…ÙˆØ¯
            new_name = st.text_input(f"Enter new name for `{col_to_rename}`:")

            if st.button("Rename Column"):
                if new_name:
                    # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØºÙŠÙŠØ± Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù…ÙˆØ¯ ÙÙŠ Ø§Ù„Ù€ DataFrame
                    df = af.rename_columns(df, {col_to_rename: new_name})
                    st.session_state.df = df
                    st.success(f"âœ… Column `{col_to_rename}` renamed to `{new_name}`.")
                    st.dataframe(df)  # Ø¹Ø±Ø¶ Ø§Ù„Ù€ DataFrame Ø¨Ø¹Ø¯ Ø§Ù„ØªØºÙŠÙŠØ±
                else:
                    st.error("âš ï¸ Please provide a new name for the column.")

        elif option == "ğŸ§  Handle Outliers":
            st.subheader("ğŸ§  Handle Outliers")

            # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø°ÙŠ Ø³ÙŠØªÙ… ØªØ·Ø¨ÙŠÙ‚Ù‡ Ø¹Ù„ÙŠÙ‡
            col_to_check = st.selectbox("Choose column to check for outliers:", df.columns)

            # Ø§Ø®ØªÙŠØ§Ø± Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ù€ outliers
            method = st.selectbox("Outlier detection method", ['IQR', 'zscore', 'isolation_forest'])

            # ÙˆØ¶Ø¹ Ø²Ø± Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù€ outliers
            if st.button("Handle Outliers"):
                if col_to_check and method:
                    # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙƒØ´Ù ÙˆØ§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù€ outliers
                    df = af.handle_outliers(df, col_to_check, method)
                    st.session_state.df = df
                    st.success(f"âœ… Outliers handled in column `{col_to_check}` using `{method}` method.")
                    st.dataframe(df)  # Ø¹Ø±Ø¶ Ø§Ù„Ù€ DataFrame Ø¨Ø¹Ø¯ Ø§Ù„ØªØºÙŠÙŠØ±
                else:
                    st.error("âš ï¸ Please select a column and a method for outlier handling.")

        elif option == "ğŸ§¾ Check Data Integrity":
            result = af.check_data_integrity(df)
            st.subheader("ğŸ“Š Data Integrity Report")
            for key, value in result.items():
                st.write(f"**{key}:** {value}")


        elif option == "ğŸ“¥ Download Cleaned Dataset":
            st.subheader("ğŸ“¥ Download Cleaned Dataset")
            format = st.selectbox("Choose format", ['csv', 'excel'])
        
            if format == "csv":
                csv = df.to_csv(index=False).encode('utf-8')
                st.download_button(
                    label="ğŸ“¥ Download CSV",
                    data=csv,
                    mime='text/csv'
                )
            elif format == "excel":
                excel_buffer = af.convert_df_to_excel(df)
                st.download_button(
                    label="ğŸ“¥ Download Excel",
                    data=excel_buffer,
                    mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
                )
        elif option == "ğŸ”„ Replace Values":
            st.subheader("ğŸ”„ Replace Values in Columns")

            # 1. Let user pick one or more columns
            cols = st.multiselect("Select column(s) to modify:", df.columns.tolist())

            # 2. Ask for the exact value to replace, and its replacement
            to_replace = st.text_input("Value to replace (exact match):")
            replacement = st.text_input("Replacement value:")

            # 3. Only run when user clicks the button
            if st.button("Apply Replacement"):
                df = af.replace_values_in_columns(df, cols, to_replace, replacement)
                st.success(f"Replaced **{to_replace}** with **{replacement}** in {len(cols)} column(s).")
                st.dataframe(df.head())
        elif option == "ğŸ”¢ Scale/Normalize Features":
            st.subheader("ğŸ”¢ Scale or Normalize Features")
            num_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()

            if st.checkbox("Select all numerical columns"):
                columns = st.multiselect("Select columns to scale/normalize", num_cols, default=num_cols)
            else:
                columns = st.multiselect("Select columns to scale/normalize", num_cols)

            method = st.selectbox("Scaling method:", ["minmax", "standard", "robust"])

            if st.button("Apply Scaling/Normalization"):
                df = af.scale_features(df, columns, method)
                st.session_state.df = df
                st.dataframe(df)



    # ==============================
    # 2. ØªØ¨ÙˆÙŠØ¨ Visualization
    # ==============================
    with tabs[1]:
        st.header("ğŸ“Š Visualization")
        
        # Select the type of plot
        plot_type = st.selectbox("Choose the type of plot", [
            "Scatter Plot", "Line Plot (Time Series)", 
            "Correlation Heatmap", "Pairplot", "Histogram", "Density Plot", 
            "Boxplot", "Violin Plot", "Bar Plot", "Pie Chart", "Missing Heatmap", 
            "Missing Barplot", "Word Cloud"
        ])
        
        # Function to show appropriate columns for each plot
        def get_column_options(plot_type, df):
            if plot_type in ["Scatter Plot", "Line Plot (Time Series)"]:
                return df.columns
            else:
                return df.select_dtypes(include=['float64', 'int64', 'object']).columns

        columns_for_plot = get_column_options(plot_type, df)

        # Handle the cases where multiple columns need to be selected (for Scatter, Line)
        if plot_type in ["Scatter Plot", "Line Plot (Time Series)"]:
            x_column = st.selectbox(f"Choose the X-axis column for {plot_type}", columns_for_plot)
            y_column = st.selectbox(f"Choose the Y-axis column for {plot_type}", columns_for_plot)

        # Handle the plots that need just one column
        elif plot_type in ["Histogram", "Density Plot", "Boxplot", "Violin Plot", "Bar Plot", "Pie Chart"]:
            selected_column = st.selectbox(f"Choose a column for the {plot_type}", columns_for_plot)

        # Function to save the plot
        def save_plot(fig, plot_type):
            buf = BytesIO()
            fig.savefig(buf, format="png")
            buf.seek(0)
            return buf

        # Handling different plot types
        if st.button(f"Generate {plot_type}"):
            try:
                if plot_type == "Histogram":
                    fig = v.plot_histogram(df, selected_column)
                    st.pyplot(fig)
                    plot_buffer = save_plot(fig, plot_type)
                    st.download_button("Download Plot", plot_buffer, file_name=f"{plot_type}.png", mime="image/png")

                elif plot_type == "Density Plot":
                    fig = v.plot_density(df, selected_column)
                    st.pyplot(fig)
                    plot_buffer = save_plot(fig, plot_type)
                    st.download_button("Download Plot", plot_buffer, file_name=f"{plot_type}.png", mime="image/png")

                elif plot_type == "Boxplot":
                    fig = v.plot_boxplot(df, selected_column)
                    st.pyplot(fig)
                    plot_buffer = save_plot(fig, plot_type)
                    st.download_button("Download Plot", plot_buffer, file_name=f"{plot_type}.png", mime="image/png")

                elif plot_type == "Violin Plot":
                    fig = v.plot_violin(df, selected_column)
                    st.pyplot(fig)
                    plot_buffer = save_plot(fig, plot_type)
                    st.download_button("Download Plot", plot_buffer, file_name=f"{plot_type}.png", mime="image/png")

                elif plot_type == "Bar Plot":
                    fig = v.plot_bar(df, selected_column)
                    st.pyplot(fig)
                    plot_buffer = save_plot(fig, plot_type)
                    st.download_button("Download Plot", plot_buffer, file_name=f"{plot_type}.png", mime="image/png")

                elif plot_type == "Pie Chart":
                    fig = v.plot_pie(df, selected_column)
                    st.pyplot(fig)
                    plot_buffer = save_plot(fig, plot_type)
                    st.download_button("Download Plot", plot_buffer, file_name=f"{plot_type}.png", mime="image/png")

                elif plot_type == "Scatter Plot":
                    fig = v.plot_scatter(df, x_column, y_column)
                    st.pyplot(fig)
                    plot_buffer = save_plot(fig, plot_type)
                    st.download_button("Download Plot", plot_buffer, file_name=f"{plot_type}.png", mime="image/png")

                elif plot_type == "Correlation Heatmap":
                    fig = v.plot_correlation_heatmap(df)
                    st.pyplot(fig)
                    plot_buffer = save_plot(fig, plot_type)
                    st.download_button("Download Plot", plot_buffer, file_name=f"{plot_type}.png", mime="image/png")

                elif plot_type == "Pairplot":
                    fig = v.plot_pairplot(df)
                    st.pyplot(fig)
                    plot_buffer = save_plot(fig, plot_type)
                    st.download_button("Download Plot", plot_buffer, file_name=f"{plot_type}.png", mime="image/png")

                elif plot_type == "Line Plot (Time Series)":
                    fig = v.plot_line(df, x_column, y_column)
                    st.pyplot(fig)
                    plot_buffer = save_plot(fig, plot_type)
                    st.download_button("Download Plot", plot_buffer, file_name=f"{plot_type}.png", mime="image/png")
                elif plot_type == "Missing Heatmap":
                    fig = v.plot_missing_heatmap(df)
                    st.pyplot(fig)
                    plot_buffer = save_plot(fig, plot_type)
                    st.download_button("Download Plot", plot_buffer, file_name=f"{plot_type}.png", mime="image/png")

                elif plot_type == "Missing Barplot":
                    fig = v.plot_missing_barplot(df)
                    st.pyplot(fig)
                    plot_buffer = save_plot(fig, plot_type)
                    st.download_button("Download Plot", plot_buffer, file_name=f"{plot_type}.png", mime="image/png")

                elif plot_type == "Word Cloud":
                    text = ' '.join(df.select_dtypes(include=['object']).fillna('').apply(lambda x: ' '.join(x), axis=1))
                    fig = v.plot_wordcloud(text)
                    st.pyplot(fig)
                    plot_buffer = save_plot(fig, plot_type)
                    st.download_button("Download Plot", plot_buffer, file_name=f"{plot_type}.png", mime="image/png")
            
            except Exception as e:
                st.error(f"An error occurred: {e}")

    # ==============================
    # 3. ØªØ¨ÙˆÙŠØ¨ Models
    # ==============================import streamlit as st

    from sklearn.linear_model import LogisticRegression, LinearRegression
    from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor
    from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
    from sklearn.svm import SVC, SVR
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, r2_score, mean_absolute_error

    import xgboost as xgb
    from xgboost import XGBClassifier, XGBRegressor

    # ========= Model Selection & Training =========
    with tabs[2]:
        st.header("ğŸ¤– Model Selection & Training")

        if not all(k in st.session_state for k in ['X_train', 'X_test', 'y_train', 'y_test', 'target_col']):
            st.warning("âš ï¸ Please complete preprocessing steps first: Set target variable and split dataset.")
        else:
            # ========== Load Models ==========
            st.subheader("ğŸ“‚ Load Pre-trained Models")
            uploaded_model = st.file_uploader("Upload a pre-trained model", type=["pkl"])

            if uploaded_model is not None:
                try:
                    model_bytes = uploaded_model.read()
                    loaded_model = pickle.loads(model_bytes)
                    st.session_state.trained_model = loaded_model
                    st.success("âœ… Model loaded successfully!")
                    
                    if st.button("ğŸ”® Predict with Loaded Model"):
                        y_pred = loaded_model.predict(st.session_state.X_test)
                        st.session_state.y_pred = y_pred
                        st.write("**Predictions**", pd.DataFrame({
                            "True Values": st.session_state.y_test,
                            "Predictions": y_pred
                        }))

                        st.subheader("ğŸ“Š Evaluation Metrics")
                        if isinstance(loaded_model, (LogisticRegression, RandomForestClassifier, KNeighborsClassifier, SVC, DecisionTreeClassifier, GradientBoostingClassifier, XGBClassifier)):
                            if isinstance(st.session_state.y_test.iloc[0], (int, str)):
                                st.write("**Accuracy:**", accuracy_score(st.session_state.y_test, y_pred))
                                st.write("**F1 Score:**", f1_score(st.session_state.y_test, y_pred, average='weighted'))
                                st.write("**Precision:**", precision_score(st.session_state.y_test, y_pred, average='weighted'))
                                st.write("**Recall:**", recall_score(st.session_state.y_test, y_pred, average='weighted'))
                            else:
                                st.error("âŒ Error: Target variable is not categorical. Please ensure you're using a classification model with categorical targets.")
                        
                        elif isinstance(loaded_model, (LinearRegression, RandomForestRegressor, KNeighborsRegressor, SVR, GradientBoostingRegressor, XGBRegressor)):
                            st.write("**RÂ² Score:**", r2_score(st.session_state.y_test, y_pred))
                            st.write("**MAE:**", mean_absolute_error(st.session_state.y_test, y_pred))
                        else:
                            st.error("âŒ Unsupported model type for evaluation.")
                except Exception as e:
                    st.error(f"âŒ Error loading model: {e}")

            # ========== Train New Model ========== 
            st.subheader("ğŸ”„ Train New Model")

            task_type = st.radio("Select ML Task Type:", ["Classification", "Regression"])

            if task_type == "Classification":
                model_name = st.selectbox("Choose a classification model:", [
                    "Logistic Regression", "Random Forest", "K-Nearest Neighbors", "Support Vector Machine", 
                    "Decision Tree", "Gradient Boosting", "XGBoost"
                ])
            else:
                model_name = st.selectbox("Choose a regression model:", [
                    "Linear Regression", "Random Forest Regressor", "K-Nearest Neighbors Regressor", 
                    "Support Vector Regressor", "Gradient Boosting Regressor", "XGBoost Regressor"
                ])

            if st.button("ğŸš€ Train Model"):
                if task_type == "Classification":
                    if model_name == "Logistic Regression":
                        model = LogisticRegression(max_iter=1000)
                    elif model_name == "Random Forest":
                        model = RandomForestClassifier()
                    elif model_name == "K-Nearest Neighbors":
                        model = KNeighborsClassifier()
                    elif model_name == "Support Vector Machine":
                        model = SVC(probability=True)
                    elif model_name == "Decision Tree":
                        model = DecisionTreeClassifier()
                    elif model_name == "Gradient Boosting":
                        model = GradientBoostingClassifier()
                    elif model_name == "XGBoost":
                        model = XGBClassifier()
                else:
                    if model_name == "Linear Regression":
                        model = LinearRegression()
                    elif model_name == "Random Forest Regressor":
                        model = RandomForestRegressor()
                    elif model_name == "K-Nearest Neighbors Regressor":
                        model = KNeighborsRegressor()
                    elif model_name == "Support Vector Regressor":
                        model = SVR()
                    elif model_name == "Gradient Boosting Regressor":
                        model = GradientBoostingRegressor()
                    elif model_name == "XGBoost Regressor":
                        model = XGBRegressor()

                model.fit(st.session_state.X_train, st.session_state.y_train)
                y_pred = model.predict(st.session_state.X_test)

                st.session_state.trained_model = model
                st.session_state.y_pred = y_pred
                st.success(f"âœ… {model_name} trained successfully!")

                # ========== Ø¹Ø±Ø¶ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© ÙˆØ§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© ==========
                st.write("### ğŸ” True vs Predicted Values")
                st.write(pd.DataFrame({
                    "True Values": st.session_state.y_test,
                    "Predicted Values": y_pred
                }))

                # ========== Evaluation ==========
                st.subheader("ğŸ“Š Evaluation Metrics")
                if task_type == "Classification":
                    st.write("**Accuracy:**", accuracy_score(st.session_state.y_test, y_pred))
                    st.write("**F1 Score:**", f1_score(st.session_state.y_test, y_pred, average='weighted'))
                    st.write("**Precision:**", precision_score(st.session_state.y_test, y_pred, average='weighted'))
                    st.write("**Recall:**", recall_score(st.session_state.y_test, y_pred, average='weighted'))
                else:
                    st.write("**RÂ² Score:**", r2_score(st.session_state.y_test, y_pred))
                    st.write("**MAE:**", mean_absolute_error(st.session_state.y_test, y_pred))

                # ========== Save and Download ==========
                model_filename = f"{model_name}.pkl"
                model_path = os.path.join("models", model_filename)
                os.makedirs(os.path.dirname(model_path), exist_ok=True)

                with open(model_path, 'wb') as f:
                    pickle.dump(model, f)
                
                buffer = io.BytesIO()
                pickle.dump(model, buffer)
                buffer.seek(0)
                st.download_button(
                    label="ğŸ“¥ Download Trained Model",
                    data=buffer,
                    file_name=model_filename,
                    mime="application/octet-stream"
                )
                
